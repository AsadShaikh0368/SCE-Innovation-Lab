{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQoQvIIaAwTV",
        "outputId": "0c103f70-7752-4c98-ee72-272efaa0dc4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=3ec55ed3420eefb0c5b6ba7ab0fde5f4b42611e80a3e7168107bd5a05aad6eab\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9XjFS5V_x91",
        "outputId": "e3be2687-586a-4f05-ba0b-dc8c9871125d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------------+----------------------+\n",
            "|      date|silo_wt_in_tons|mtd_running_total_tons|\n",
            "+----------+---------------+----------------------+\n",
            "|06-01-2023|            415|                   415|\n",
            "|06-02-2023|            415|                   830|\n",
            "|06-03-2023|            415|                  1245|\n",
            "|06-04-2023|            415|                  1660|\n",
            "|06-05-2023|            415|                  2075|\n",
            "|06-06-2023|            415|                  2490|\n",
            "|06-07-2023|            415|                  2905|\n",
            "|06-08-2023|            415|                  3320|\n",
            "|06-09-2023|            415|                  3735|\n",
            "|06-10-2023|            415|                  4150|\n",
            "|06-11-2023|            415|                  4565|\n",
            "|06-12-2023|            415|                  4980|\n",
            "|06-13-2023|            415|                  5395|\n",
            "|06-14-2023|            415|                  5810|\n",
            "|06-15-2023|            415|                  6225|\n",
            "|06-16-2023|            415|                  6640|\n",
            "|06-17-2023|            415|                  7055|\n",
            "|06-18-2023|            415|                  7470|\n",
            "|06-19-2023|            415|                  7885|\n",
            "|06-20-2023|            415|                  8300|\n",
            "|06-21-2023|            415|                  8715|\n",
            "|06-22-2023|            415|                  9130|\n",
            "|06-23-2023|            415|                  9545|\n",
            "|06-24-2023|            415|                  9960|\n",
            "|06-25-2023|            415|                 10375|\n",
            "|06-26-2023|            415|                 10790|\n",
            "|06-27-2023|            415|                 11205|\n",
            "|06-28-2023|            415|                 11620|\n",
            "|06-29-2023|            415|                 12035|\n",
            "|06-30-2023|            415|                 12450|\n",
            "+----------+---------------+----------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum, monotonically_increasing_id, split, substring, array_contains, to_timestamp, lit, array, expr\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Load Actual Silo Readings\n",
        "df_actuals = spark.read.csv(\"/content/silo_actuals (1).csv\", sep=\",\", header=True, inferSchema=True)\n",
        "\n",
        "# Load Reference Tons from historical daily\n",
        "df_historical = spark.read.csv(\"/content/historical_averages (1).csv\", sep=\",\", header=True, inferSchema=True)\n",
        "\n",
        "# Create a dataframe of all days in June 2023\n",
        "days_list = [\n",
        "    \"06-01-2023\", \"06-02-2023\", \"06-03-2023\", \"06-04-2023\", \"06-05-2023\", \"06-06-2023\", \"06-07-2023\",\n",
        "    \"06-08-2023\", \"06-09-2023\", \"06-10-2023\", \"06-11-2023\", \"06-12-2023\", \"06-13-2023\", \"06-14-2023\",\n",
        "    \"06-15-2023\", \"06-16-2023\", \"06-17-2023\", \"06-18-2023\", \"06-19-2023\", \"06-20-2023\", \"06-21-2023\",\n",
        "    \"06-22-2023\", \"06-23-2023\", \"06-24-2023\", \"06-25-2023\", \"06-26-2023\", \"06-27-2023\", \"06-28-2023\",\n",
        "    \"06-29-2023\", \"06-30-2023\"\n",
        "]\n",
        "\n",
        "# Create a DataFrame with the days list\n",
        "df_days = spark.createDataFrame([(day,) for day in days_list], [\"date\"])\n",
        "\n",
        "# Join the Actual Silo Readings dataframe with the days dataframe\n",
        "df_actuals = df_actuals.join(df_days, on=\"date\", how=\"right\")\n",
        "\n",
        "# Fill in missing values with reference tons\n",
        "df_actuals = df_actuals.fillna(0).replace(0, df_historical.select(sum(\"average_tons\")).collect()[0][0]).withColumnRenamed(\"sum(average_tons)\", \"average_tons\")\n",
        "\n",
        "# Calculate the running total of tons for each day\n",
        "window = Window.orderBy(\"date\").rowsBetween(Window.unboundedPreceding, 0)\n",
        "df_actuals = df_actuals.withColumn(\"mtd_running_total_tons\", sum(\"silo_wt_in_tons\").over(window))\n",
        "\n",
        "# Save the output to a csv file\n",
        "df_actuals.write.csv(\"/content/output1_reference.csv\", mode=\"overwrite\", header=True)\n",
        "\n",
        "# Show the top 10 rows of the output\n",
        "df_actuals.show(31)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dnd6fZcp_ylu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}